{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 1000\n",
    "#df_train.loc[df_train[\"LGBTI\"] > 0, [\"title\", \"text\", \"text_label\"]].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5670, 23)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "files = glob(\"../data/test*beto*.csv\")\n",
    "\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "\n",
    "\n",
    "df_test = pd.concat(dfs)\n",
    "\n",
    "df_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       WOMEN       0.58      0.34      0.43       131\n",
      "       LGBTI       0.59      0.38      0.46        88\n",
      "      RACISM       0.80      0.70      0.75       230\n",
      "       CLASS       0.73      0.47      0.58        76\n",
      "    POLITICS       0.76      0.49      0.60       144\n",
      "    DISABLED       0.63      0.58      0.60        66\n",
      "  APPEARANCE       0.81      0.72      0.76       189\n",
      "    CRIMINAL       0.78      0.65      0.71       185\n",
      "\n",
      "   micro avg       0.75      0.58      0.65      1109\n",
      "   macro avg       0.71      0.54      0.61      1109\n",
      "weighted avg       0.74      0.58      0.64      1109\n",
      " samples avg       0.10      0.09      0.10      1109\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/jmperez/projects/rioplatense_hate_speech/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/users/jmperez/projects/rioplatense_hate_speech/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from rioplatense_hs.preprocessing import text_to_label, labels\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred_labels = [f\"PRED_{label}\" for label in labels]\n",
    "gold_labels = df_test[labels]\n",
    "preds = (df_test[pred_labels] > 0.5).astype(int)\n",
    "\n",
    "\n",
    "print(classification_report(gold_labels, preds, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.96      0.95      4794\n",
      "        True       0.77      0.67      0.72       876\n",
      "\n",
      "    accuracy                           0.92      5670\n",
      "   macro avg       0.86      0.82      0.84      5670\n",
      "weighted avg       0.92      0.92      0.92      5670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels = [l for l in df_test.columns if l.startswith(\"PRED\")]\n",
    "\n",
    "label_hate = df_test[labels].sum(axis=1) > 0\n",
    "pred_hate = (df_test[pred_labels] > 0.5).sum(axis=1) > 0\n",
    "\n",
    "print(classification_report(label_hate, pred_hate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"../data/test_beto.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
